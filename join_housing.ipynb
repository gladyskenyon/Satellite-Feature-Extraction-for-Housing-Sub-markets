{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ca58f15-4f9d-4497-84d8-a0cda83ebb30",
   "metadata": {},
   "source": [
    "### External validation of clusters using housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d0a7c657-75fc-46df-bea5-eabb25b2ed54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "from shapely.geometry import Point\n",
    "from shapely import wkt\n",
    "import numpy as np\n",
    "import shapely\n",
    "from shapely.geometry import shape\n",
    "from shapely.geometry import Point, box\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46544635-aa34-41b1-8cf9-e837643a1e88",
   "metadata": {},
   "source": [
    "### External Validation using housing listings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5768d0b6-91ec-40e5-a59a-a125d06bc29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in labels saved from feature extraction\n",
    "# Create an empty list to store the DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Loop through the range of 10 (0 to 9) to read in the CSV files\n",
    "for i in range(10):\n",
    "    filename = f'labels_df{i}.csv'  # Generate the file name\n",
    "    df = pd.read_csv(filename, index_col=0)  # Read the CSV file into a DataFrame and set the first column as the index\n",
    "    # Drop the first column (assuming it's the index column)\n",
    "#    df = df.iloc[:, 1:]  # Assuming the first column is index and we drop it\n",
    "    dataframes.append(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b1695710-ec9f-4a04-81fc-840fec5cf01d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in shapefile of Madrid with district boundaries\n",
    "db = gpd.read_file('Distritos.shp')\n",
    "# Set local crs in m \n",
    "db = db.to_crs(25830)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5a1a3ed1-fd1b-40c0-bb0b-d0da2609a77c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize grid_list\n",
    "grid_list = []\n",
    "\n",
    "# Set the initial CRS and step size\n",
    "crs = '25830'\n",
    "initial_step = 300\n",
    "\n",
    "# Total bounds of the original GeoDataFrame (assuming it's called db)\n",
    "a, b, c, d = db.total_bounds\n",
    "\n",
    "# Create a grid for geometry with step size 300\n",
    "gdf_grid = gpd.GeoDataFrame(\n",
    "    geometry=[\n",
    "        shapely.geometry.box(minx, miny, maxx, maxy)\n",
    "        for minx, maxx in zip(np.arange(a, c, initial_step), np.arange(a, c, initial_step)[1:])\n",
    "        for miny, maxy in zip(np.arange(b, d, initial_step), np.arange(b, d, initial_step)[1:])\n",
    "    ],\n",
    "    crs=crs,\n",
    ").to_crs(db.crs)\n",
    "\n",
    "# Append the grid to grid_list\n",
    "grid_list.append(gdf_grid)\n",
    "\n",
    "\n",
    "# Create grids to patch image\n",
    "# Change the step size to change sizes\n",
    "# Set the initial CRS and step size\n",
    "crs = '25830'\n",
    "initial_step = 500\n",
    "final_step = 4000\n",
    "step_increase = 400\n",
    "\n",
    "\n",
    "# Loop through step sizes\n",
    "for STEP in range(initial_step, final_step + 1, step_increase):\n",
    "    # Total bounds of the original GeoDataFrame (assuming it's called db)\n",
    "    a, b, c, d = db.total_bounds\n",
    "\n",
    "    # Create a grid for geometry\n",
    "    gdf_grid = gpd.GeoDataFrame(\n",
    "        geometry=[\n",
    "            shapely.geometry.box(minx, miny, maxx, maxy)\n",
    "            for minx, maxx in zip(np.arange(a, c, STEP), np.arange(a, c, STEP)[1:])\n",
    "            for miny, maxy in zip(np.arange(b, d, STEP), np.arange(b, d, STEP)[1:])\n",
    "        ],\n",
    "        crs=crs,\n",
    "    ).to_crs(db.crs)\n",
    "\n",
    "    # Append the current GeoDataFrame to the list\n",
    "    grid_list.append(gdf_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5ed21e56-1674-4b17-b5c8-30031e0089fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in housing data\n",
    "housep = pd.read_csv('Madrid_Sale.csv')\n",
    "# Assuming 'long' and 'lat' are columns in your DataFrame 'housep'\n",
    "housep['long'] = pd.to_numeric(housep['long'])\n",
    "housep['lat'] = pd.to_numeric(housep['lat'])\n",
    "\n",
    "# Create the 'geometry' column with Point geometries\n",
    "housep['geometry'] = [Point(xy) for xy in zip(housep['long'], housep['lat'])]\n",
    "# Create a GeoDataFrame\n",
    "sale = gpd.GeoDataFrame(housep, crs=\"EPSG:4326\", geometry=housep['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "39ee9015-78eb-495b-909a-915f0edb32f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop duplicate listings with repeated ID\n",
    "sale = sale.sort_values(by='period')  # Ensure the DataFrame is sorted by 'period' in ascending order\n",
    "sale = sale.drop_duplicates(subset='ID', keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4fba7cd2-d4f8-447c-89c2-bf7572362fe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change crs\n",
    "sale = sale.to_crs('25830')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "728cc78d-c872-4d84-99f8-28b42a04a8cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add scale value to labels \n",
    "# Initialize the scale value for the first DataFrame\n",
    "scale_value = 30\n",
    "\n",
    "# Iterate over each DataFrame in the dataframes list\n",
    "for idx, df in enumerate(dataframes):\n",
    "    # Add a new column named 'scale' with the corresponding value\n",
    "    df['scale'] = scale_value\n",
    "    \n",
    "    # Update the scale value for the next DataFrame\n",
    "    if idx == 0:\n",
    "        scale_value += 20  # Increase by 20 for the second DataFrame\n",
    "    else:\n",
    "        scale_value += 40  # Increase by 40 for each subsequent DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4fbfde82-9ff6-4610-b296-763b5e6a7097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do spatial join between matching grid list and labels for each scale\n",
    "# Create an empty list to store the merged DataFrames\n",
    "merged_dfs = []\n",
    "\n",
    "# Iterate over grid_list_dbs and dataframes simultaneously\n",
    "for gdf_grid, df in zip(grid_list, dataframes):\n",
    "    # Perform a spatial join using the index\n",
    "    merged_df = pd.merge(gdf_grid, df,left_index=True, right_index=True, how='left')\n",
    "    # Append the merged DataFrame to the list\n",
    "    merged_dfs.append(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b978182d-c75b-46d9-9965-88e1ab4bdab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert these merged_dfs to geodataframe\n",
    "\n",
    "# Create an empty list to store the converted GeoDataFrames\n",
    "converted_dfs = []\n",
    "\n",
    "# Iterate over each merged DataFrame\n",
    "for merged_df in merged_dfs:\n",
    "    # Create a GeoDataFrame from the DataFrame\n",
    "    converted_df = gpd.GeoDataFrame(merged_df, geometry='geometry')\n",
    "    # Set the CRS to EPSG:25830\n",
    "    converted_df.crs = 'EPSG:25830'\n",
    "    # Append the converted GeoDataFrame to the list\n",
    "    converted_dfs.append(converted_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "cb043c92-c98e-4605-8632-63cd94e31693",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    }
   ],
   "source": [
    "# Do spatial join with housing data\n",
    "# Create an empty list to store the spatially joined GeoDataFrames\n",
    "spatial_joined_dfs = []\n",
    "\n",
    "# Iterate over each converted GeoDataFrame\n",
    "for converted_df in converted_dfs:\n",
    "    # Perform a spatial join with the 'sale' GeoDataFrame\n",
    "    spatial_joined_df = gpd.sjoin(converted_df, sale, how='inner', op='intersects')\n",
    "    # Append the spatially joined GeoDataFrame to the list\n",
    "    spatial_joined_dfs.append(spatial_joined_df)\n",
    "\n",
    "# Now, spatial_joined_dfs contains the result of the spatial join of each item in converted_dfs with the 'sale' GeoDataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6576380e-b7a2-4571-85c4-14a073c0b35f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Directory to save the spatially joined GeoDataFrames\n",
    "output_dir = \"spatial_joined_data\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over each spatially joined GeoDataFrame\n",
    "for idx, spatial_joined_df in enumerate(spatial_joined_dfs):\n",
    "    # Construct the filename for saving\n",
    "    filename = os.path.join(output_dir, f\"spatial_joined_df_{idx}.csv\")\n",
    "    \n",
    "    # Save the spatially joined GeoDataFrame to a CSV file\n",
    "    spatial_joined_df.to_csv(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ccf93de1-14bd-469d-8dde-f4eaeb04a043",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the std. dev in housing characteristics\n",
    "# Assuming you have a list of dataframes named spatial_joined_dfs\n",
    "\n",
    "# Initialize an empty list to store the dataframes\n",
    "dfs_list = []\n",
    "\n",
    "# Iterate over each DataFrame in spatial_joined_dfs\n",
    "for idx, df in enumerate(spatial_joined_dfs):\n",
    "    # Initialize a dictionary to store total standard deviations for each DataFrame\n",
    "    total_std_devs = {}\n",
    "    \n",
    "    # Iterate over each column except the first (assuming the first column is 'geometry')\n",
    "    for column in df.columns[1:17]:\n",
    "        # Group the DataFrame by the current column\n",
    "        grouped = df.groupby(column)\n",
    "        \n",
    "        # Calculate the standard deviation for 'price', 'area', 'conyr', 'maxfloor', 'dcount', 'dist_city_centre', and 'room' columns\n",
    "        std_price = grouped['price'].std().sum()\n",
    "        std_area = grouped['area'].std().sum()\n",
    "        std_conyr = grouped['conyr'].std().sum()\n",
    "        std_mxfloor = grouped['maxfloor'].std().sum()\n",
    "        std_dcount = grouped['dcount'].std().sum()\n",
    "        std_distc = grouped['dist_city_centre'].std().sum()\n",
    "        std_room = grouped['room'].std().sum()\n",
    "\n",
    "        \n",
    "        # Store the total standard deviations in the dictionary\n",
    "        total_std_devs[column] = {'std_price': std_price, 'std_area': std_area, 'std_conyr': std_conyr,\n",
    "                                  'std_mxfloor': std_mxfloor, 'std_dcount': std_dcount, 'std_distc': std_distc,\n",
    "                                  'std_room': std_room}  # Add 'std_scale'\n",
    "\n",
    "    # Create a DataFrame from the dictionary\n",
    "    df_temp = pd.DataFrame(total_std_devs).T.reset_index().rename(columns={'index': 'Column'})\n",
    "    \n",
    "    # Add 'DataFrame' column\n",
    "    df_temp['DataFrame'] = f'DF_{idx}'\n",
    "    \n",
    "    # Append the DataFrame to the list\n",
    "    dfs_list.append(df_temp)\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "df = pd.concat(dfs_list, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b7ddef8b-eb77-4d69-9f45-6a06f138ed50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create number of feats variable \n",
    "# Extract the first number from the 'Column' column\n",
    "df['num_feats'] = df['Column'].apply(lambda x: int(x.split('_')[0]))\n",
    "# Extract the number of clusters from the 'Column' column\n",
    "df['num_clus'] = df['Column'].apply(lambda x: int(x.split('_')[1]))\n",
    "\n",
    "# Convert the 'num_feats' and 'num_clus' columns to integers\n",
    "df['num_feats'] = df['num_feats'].astype(int)\n",
    "df['num_clus'] = df['num_clus'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ee52e2f2-14b9-4329-93d2-9cc5427ae7fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a scale variable\n",
    "# Define the mapping dictionary for scale values\n",
    "scale_mapping = {\n",
    "    'DF_0': 30,\n",
    "    'DF_1': 50,\n",
    "    'DF_2': 90,\n",
    "    'DF_3': 130,\n",
    "    'DF_4': 170,\n",
    "    'DF_5': 210,\n",
    "    'DF_6': 250,\n",
    "    'DF_7': 290,\n",
    "    'DF_8': 330,\n",
    "    'DF_9': 370\n",
    "}\n",
    "\n",
    "# Map the 'DataFrame' column to scale values using the mapping dictionary\n",
    "df['scale'] = df['DataFrame'].map(scale_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9663ee0a-0b5b-4673-be68-3aec338a97fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Divide the standard deviation in 'price', 'area', and 'conyr' by the number of clusters to get the average std. dev per clusters\n",
    "df['std_price_divided'] = df['std_price'] / df['num_clus']\n",
    "df['std_area_divided'] = df['std_area'] / df['num_clus']\n",
    "df['std_conyr_divided'] = df['std_conyr'] / df['num_clus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "46b20878-f21a-467a-b9b5-73478b4ac682",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv('housing_scores2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
